{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI3I0zV6ILTe",
        "outputId": "538613d3-bae7-4617-e4db-652ef9b63906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Unzipping directly to Colab temporary storage (faster access)\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/trum_tweet_sentiment_analysis (1).zip\" -d \"/content\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwstaNHoIPZU",
        "outputId": "ed1bc5d0-801e-468d-b846-a3f97eeb84eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/trum_tweet_sentiment_analysis (1).zip\n",
            "  inflating: /content/trum_tweet_sentiment_analysis (1).csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Assuming the file is in your Drive and you know the path\n",
        "file_path = '/content/drive/MyDrive/trum_tweet_sentiment_analysis (1).zip'  # Update this path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTAliNIfIRnF",
        "outputId": "3cd251c0-67a9-436a-c179-301afd410d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  Sentiment\n",
            "0  RT @JohnLeguizamo: #trump not draining swamp b...          0\n",
            "1  ICYMI: Hackers Rig FM Radio Stations To Play A...          0\n",
            "2  Trump protests: LGBTQ rally in New York https:...          1\n",
            "3  \"Hi I'm Piers Morgan. David Beckham is awful b...          0\n",
            "4  RT @GlennFranco68: Tech Firm Suing BuzzFeed fo...          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "# Check columns and data\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nColumn names:\", df.columns)\n",
        "print(\"\\nSentiment distribution:\")  # Changed label to Sentiment\n",
        "print(df['Sentiment'].value_counts())  # Changed label to Sentiment\n",
        "\n",
        "# For later in your code where you split data:\n",
        "X = df['text']          # Features (text column)\n",
        "y = df['Sentiment']      # Labels (now using correct Sentiment column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSZKh5wNIWyd",
        "outputId": "db77ec9c-cb25-4fe7-f4fe-6217cf69935e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1850123, 2)\n",
            "\n",
            "First 5 rows:\n",
            "                                                text  Sentiment\n",
            "0  RT @JohnLeguizamo: #trump not draining swamp b...          0\n",
            "1  ICYMI: Hackers Rig FM Radio Stations To Play A...          0\n",
            "2  Trump protests: LGBTQ rally in New York https:...          1\n",
            "3  \"Hi I'm Piers Morgan. David Beckham is awful b...          0\n",
            "4  RT @GlennFranco68: Tech Firm Suing BuzzFeed fo...          0\n",
            "\n",
            "Column names: Index(['text', 'Sentiment'], dtype='object')\n",
            "\n",
            "Sentiment distribution:\n",
            "Sentiment\n",
            "0    1244211\n",
            "1     605912\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# First: Download ALL required NLTK data in GPU runtime\n",
        "nltk.download('all', quiet=True)  # This will download all NLTK resources\n",
        "\n",
        "# Then download specific resources we need\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)  # Specifically for Punkt tokenizer tables\n",
        "\n",
        "def preprocess_text(text, lemmatize=True):\n",
        "    \"\"\"\n",
        "    Text preprocessing pipeline:\n",
        "    - Lowercase\n",
        "    - Remove URLs, mentions, punctuation\n",
        "    - Remove stopwords\n",
        "    - Tokenization & Lemmatization\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove URLs, mentions (@user), hashtags\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+|#\\w+', '', text)\n",
        "\n",
        "        # Remove punctuation & special chars\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # Lemmatization\n",
        "        if lemmatize and tokens:\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        # Remove short words\n",
        "        tokens = [word for word in tokens if len(word) > 1]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {str(e)[:100]}...\")  # Print first 100 chars of error\n",
        "        return \"\"\n",
        "\n",
        "# Apply preprocessing (without progress bar for simplicity)\n",
        "print(\"Starting text preprocessing...\")\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"\\nSample cleaned text:\")\n",
        "print(df[['text', 'cleaned_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvarWTvBIY4c",
        "outputId": "815d3306-79aa-4673-b3b4-27746895525e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting text preprocessing...\n",
            "\n",
            "Sample cleaned text:\n",
            "                                                text  \\\n",
            "0  RT @JohnLeguizamo: #trump not draining swamp b...   \n",
            "1  ICYMI: Hackers Rig FM Radio Stations To Play A...   \n",
            "2  Trump protests: LGBTQ rally in New York https:...   \n",
            "3  \"Hi I'm Piers Morgan. David Beckham is awful b...   \n",
            "4  RT @GlennFranco68: Tech Firm Suing BuzzFeed fo...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  rt draining swamp taxpayer dollar trip adverti...  \n",
            "1  icymi hacker rig fm radio station play anti tr...  \n",
            "2             trump protest lgbtq rally new york via  \n",
            "3  hi pier morgan david beckham awful donald trum...  \n",
            "4  rt tech firm suing buzzfeed publishing unverif...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test Split"
      ],
      "metadata": {
        "id": "s6MXylsxJVpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use the correct column name ('Sentiment' instead of 'label')\n",
        "X = df['cleaned_text']  # Features (processed text)\n",
        "y = df['Sentiment']     # Labels (using correct column name)\n",
        "\n",
        "# Split into 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nTrain size:\", len(X_train))\n",
        "print(\"Test size:\", len(X_test))\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "print(y_train.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRF-pEIlJKSr",
        "outputId": "17e1cd1d-058f-4fca-bd03-1811ec75613a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train size: 1480098\n",
            "Test size: 370025\n",
            "\n",
            "Class distribution in training set:\n",
            "Sentiment\n",
            "0    995648\n",
            "1    484450\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "7SVf7ujZJXYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
        "\n",
        "# Fit on training data & transform\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "\n",
        "# Transform test data (don't fit again!)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(\"\\nTF-IDF shape (Train):\", X_train_tfidf.shape)\n",
        "print(\"TF-IDF shape (Test):\", X_test_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw_TeGOEJL41",
        "outputId": "ca200c49-34cc-4f4b-99c7-55fecf6b5be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF shape (Train): (1480098, 5000)\n",
            "TF-IDF shape (Test): (370025, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training & Evaluation (Logistic Regression)"
      ],
      "metadata": {
        "id": "xF91WB7_JZz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize & train model\n",
        "model = LogisticRegression(max_iter=1000)  # Increase max_iter for convergence\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gas9bpiJOLg",
        "outputId": "7e0b2c10-8587-41e1-ae3d-18766284fcbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94    248563\n",
            "           1       0.90      0.86      0.88    121462\n",
            "\n",
            "    accuracy                           0.92    370025\n",
            "   macro avg       0.91      0.90      0.91    370025\n",
            "weighted avg       0.92      0.92      0.92    370025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save & Load Model"
      ],
      "metadata": {
        "id": "Dp7BBhKAJdEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save model & TF-IDF vectorizer\n",
        "joblib.dump(model, 'trump_sentiment_model.pkl')\n",
        "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "# Load later (for deployment)\n",
        "# model = joblib.load('trump_sentiment_model.pkl')\n",
        "# tfidf = joblib.load('tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3dAMO60JQZf",
        "outputId": "7fe77d53-4d20-41ff-a649-24cabd9a8feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}